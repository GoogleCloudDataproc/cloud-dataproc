{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "990f04c9-c585-406d-84c7-83a279f45db5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client()\n",
    "BUCKET_NAME = 'cloud-samples-data'\n",
    "bucket = client.get_bucket(BUCKET_NAME)\n",
    "\n",
    "# Limiting to only 50 images for sample\n",
    "blobs = bucket.list_blobs(prefix=\"generative-ai/image\", max_results = 50)\n",
    "blobs = filter(lambda blob: get_blob_uri(blob).endswith(\"jpg\"), list(blobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3069d5-3cbf-45dc-ba55-dc26c8bc9327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud.storage.blob import Blob\n",
    "def get_blob_uri(blob):\n",
    "    return 'gs://' + blob.id[:-(len(str(blob.generation)) + 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87ddfe59-c07a-4522-ab44-bb5f456ed8b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/11 15:56:37 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"classficationDemo\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Set to True for GPU enabled serverless sessions/dataproc clusters\n",
    "cuda = False\n",
    "\n",
    "# Enable Arrow support.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"64\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.datasets.folder import default_loader  # private API\n",
    "\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "\n",
    "use_cuda = cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "files_df = spark.createDataFrame(map(lambda file : get_blob_uri(file), blobs), StringType()).repartition(10)\n",
    "\n",
    "# Downloads and broadcasts the model weights to all the workers\n",
    "model_state = models.resnet50(pretrained=True).state_dict()\n",
    "bc_model_state = sc.broadcast(model_state)\n",
    "\n",
    "def get_model_for_eval():\n",
    "  \"\"\"Gets the broadcasted model to each python worker\"\"\"\n",
    "  torch.hub.set_dir(\"/tmp/models\")\n",
    "  model = models.resnet50(pretrained=True)\n",
    "  model.load_state_dict(bc_model_state.value)\n",
    "  model.eval()\n",
    "  return model\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "  def __init__(self, paths, transform=None):\n",
    "    self.paths = paths\n",
    "    self.transform = transform\n",
    "  def __len__(self):\n",
    "    return len(self.paths)\n",
    "  def __getitem__(self, index):\n",
    "    client = storage.Client()\n",
    "    path = self.paths[index]\n",
    "    blob = Blob.from_string(path, client=client)\n",
    "    local_file = \"/tmp/\" + path.split(\"/\")[-1]\n",
    "    blob.download_to_file(open(local_file, \"wb\"))\n",
    "    image = default_loader(local_file)\n",
    "    if self.transform is not None:\n",
    "      image = self.transform(image)\n",
    "    return image\n",
    "\n",
    "# Using Pandas UDF for parallel run on each partition\n",
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def predict_batch_udf(paths: pd.Series) -> pd.Series:\n",
    "  transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "  images = ImageDataset(paths, transform=transform)\n",
    "  loader = torch.utils.data.DataLoader(images, batch_size=500, num_workers=8)\n",
    "  model = get_model_for_eval()\n",
    "  model.to(device)\n",
    "  all_predictions = []\n",
    "  with torch.no_grad():\n",
    "    for batch in loader:\n",
    "      predictions = list(model(batch.to(device)).cpu().numpy())\n",
    "      for prediction in predictions:\n",
    "        all_predictions.append(prediction)\n",
    "  return pd.Series(all_predictions)\n",
    "\n",
    "output_file_path = \"/tmp/results\"\n",
    "predictions_df = files_df.select(col(\"value\"), predict_batch_udf(col(\"value\"))).alias(\"predictions\")\n",
    "predictions_df.write.mode(\"overwrite\").parquet(output_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa380428-ceb3-4b04-911a-2ffe1353e195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+\n",
      "|               value|predict_batch_udf(value)|\n",
      "+--------------------+------------------------+\n",
      "|gs://cloud-sample...|    [-1.4993658, -1.5...|\n",
      "|gs://cloud-sample...|    [-3.9951859, -0.9...|\n",
      "|gs://cloud-sample...|    [-2.278611, -1.12...|\n",
      "|gs://cloud-sample...|    [-2.4935248, -0.2...|\n",
      "|gs://cloud-sample...|    [-2.3286686, 0.73...|\n",
      "+--------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(output_file_path).limit(5).show()"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-deependrap-template-2",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "deependrap on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-deependrap-template-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

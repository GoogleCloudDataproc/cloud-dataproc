{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Generate Text Embeddings with Hugging face model using Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24743cf4a1e1"
   },
   "source": [
    "**_NOTE_**: This notebook has been tested in the following environment:\n",
    "\n",
    "* Python version = 3.10.13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "The example creates a similarity search on Stackoverflow questions to identify similar topics, questions and technologies being discussed. It leverages BigQuery and Dataproc Serverless for distributed prediction on Deep Learning models.\n",
    "\n",
    "Data Engineers and Data Scientists with existing working knowledge of BigQuery and Dataproc/Spark can use this notebook to launch batch inference jobs at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use Apache Spark for batch inference/prediction and BQ for Vector Search. You also learn to use Dataproc Interactive Sessions from Jupyter Notebooks - From Vertex Workbench Instance or BQStudio/Colab Enterprise\n",
    "\n",
    "The example uses open source stackoverflow data and open source Hugging Face model - all-MiniLM-L12-v2 to generate embeddings of text data. The similarity search index is created in BigQuery.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- BQML - Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "BigQuery public dataset - \"bigquery-public-data.stackoverflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Dataproc Serverless\n",
    "* BigQuery\n",
    "* Vertex Workbench Instance / BQ Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "TODO: [BigQuery pricing](https://cloud.google.com/bigquery/pricing),\n",
    "and [Dataproc Serverless Pricing](https://cloud.google.com/dataproc-serverless/pricing), \n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "[Enable the Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=dataproc.googleapis.com)\n",
    "\n",
    "//Check for Dataproc Serverless\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk).\n",
    "\n",
    "5. //check for networking requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all",
    "tags": []
   },
   "source": [
    "#### Select Dataproc Serverless Interactive Session as the Kernel for this notebook\n",
    "\n",
    "Create a [Dataproc Interactive Session Template](https://cloud.google.com/dataproc-serverless/docs/guides/create-serverless-sessions-templates) using the network configuration specified in the link.\n",
    "\n",
    "Once the Template is created, select the interactive template as the kernel for the notebook. This will create Dataproc Interactive Session [check here](https://console.cloud.google.com/dataproc/interactive?)\n",
    "\n",
    "This may take a while, so please dont close the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2b4ef9b72d43",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sentence-transformers transformers\n",
    "!pip install torchvision google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall -y numpy\n",
    "!pip install numpy==1.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to certain dependencies between Hugging Face models, we fix the numpy version to 1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please donot forget to restart the kernel now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WReHDGG5g0XY"
   },
   "source": [
    "#### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "project_id = 'google.com:hadoop-cloud-dev'  # @param {type:\"string\"}\n",
    "region = \"us-central1\"  # @param {type: \"string\"}\n",
    "\n",
    "# Set the project id\n",
    "# ! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBCra4QMA2wR",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "The Cloud SDK, code and other libraries currently run as the service account identity of the Workbench Instance running this notebook.\n",
    "\n",
    "//I dont think we need this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de775a3773ba"
   },
   "source": [
    "**- Authenticate the Cloud SDK with your credentials :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "254614fa0c46"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "authlibtitle"
   },
   "source": [
    "**- Authenticate code and libraries with your credentials :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "authlibcode"
   },
   "outputs": [],
   "source": [
    "# ! gcloud auth application-default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6b2ccc891ed"
   },
   "source": [
    "**- Service account or other**\n",
    "* See how to grant Cloud Storage permissions to your service account at https://cloud.google.com/storage/docs/gsutil/commands/iam#ch-examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct, col, array, udf, lit\n",
    "from pyspark.sql.types import ArrayType, FloatType, Union, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session & load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Embeddings\")\\\n",
    ".getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_dataset = 'jvidhi_test'\n",
    "stackoverflow_table_name = f'{bq_dataset}.stackoverflow_questions'\n",
    "stackoverflow_index = f'{bq_dataset}.stackoverflow_index'\n",
    "\n",
    "stackoverflow_data = spark.read.format('bigquery') \\\n",
    "  .option('table', 'bigquery-public-data.stackoverflow.posts_questions') \\\n",
    "  .load()\n",
    "\n",
    "stackoverflow_data = stackoverflow_data.select('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title']\n",
      "23020127\n"
     ]
    }
   ],
   "source": [
    "print(stackoverflow_data.columns)\n",
    "print(stackoverflow_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|Html.ActionLink d...|\n",
      "| Primitive recursion|\n",
      "|  While vs. Do While|\n",
      "|Protect ASP.NET S...|\n",
      "|Difference betwee...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stackoverflow_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create batch prediction function\n",
    "\n",
    "The model will be called and loaded within the batch predict function which will load the model in executors and run distributed inference on spark dataframe\n",
    "Learn more - https://spark.apache.org/docs/3.4.3/api/python/reference/api/pyspark.ml.functions.predict_batch_udf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import torch\n",
    "    from pyspark.sql.types import ArrayType, StringType\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "    model.to(device)\n",
    "    \n",
    "    def predict(inputs: ArrayType(StringType())) -> np.ndarray:\n",
    "        embeddings = model.encode(inputs) #size [batch_size]\n",
    "        return embeddings #return (batch_size,384)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_batch_udf(predict_batch_fn,\n",
    "                          return_type=ArrayType(FloatType()),\n",
    "                          batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.34 ms, sys: 3.86 ms, total: 11.2 ms\n",
      "Wall time: 49.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "prediction = stackoverflow_data.withColumn(\"embeddings\", results('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               title|          embeddings|\n",
      "+--------------------+--------------------+\n",
      "|Html.ActionLink d...|[-0.003286022, -0...|\n",
      "| Primitive recursion|[-0.099161394, 0....|\n",
      "|  While vs. Do While|[0.026201472, -0....|\n",
      "|Protect ASP.NET S...|[-0.045438357, 0....|\n",
      "|Difference betwee...|[0.08556235, -0.0...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dataframe as a table in BigQuery. We will create a vector index on this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:>                                                        (0 + 12) / 39]\r"
     ]
    }
   ],
   "source": [
    "# the whole table fails to be inserted into BigQuery\n",
    "\n",
    "prediction.limit(200000).write.mode('overwrite')\\\n",
    ".format(\"bigquery\")\\\n",
    ".option(\"table\",f\"{project_id}.{stackoverflow_table_name}_embeddings\")\\\n",
    ".option(\"writeMethod\",\"direct\")\\\n",
    ".save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector index in BigQuery \n",
    "https://cloud.google.com/bigquery/docs/vector-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    CREATE VECTOR INDEX @index ON @table_name(embeddings)\n",
    "    OPTIONS (index_type = 'TREE_AH', distance_type = 'EUCLIDEAN',\n",
    "    tree_ah_options = '{\"normalization_type\": \"L2\"}');\n",
    "\"\"\"\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"index\", \"STRING\", stackoverflow_index),\n",
    "        bigquery.ArrayQueryParameter(\"table_name\", \"STRING\", stackoverflow_table_name),\n",
    "    ]\n",
    ")\n",
    "query_job = client.query(query, job_config=job_config)  # Make an API request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate embedding of the search query which will be searched on the vector index to find similar search items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_sentence = \"Apache Spark on Dataproc\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "embeddings = model.encode(query_sentence).tolist()\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT * FROM\n",
    "      VECTOR_SEARCH( TABLE @table,'embeddings',(select @embeddings),\n",
    "        top_k => 5, options => '{\"fraction_lists_to_search\": 0.01}');\n",
    "\"\"\"\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ArrayQueryParameter(\"table\", \"STRING\", stackoverflow_table_name),\n",
    "        bigquery.ArrayQueryParameter(\"embeddings\", \"FLOAT\", embeddings),\n",
    "    ]\n",
    ")\n",
    "query_job = client.query(query, job_config=job_config)  # Make an API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in query_job:\n",
    "    print(row[1]['highlights'])\n",
    "    print(f'Distance: {row[2]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can [delete the Google Cloud\n",
    "project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) you used for the tutorial.\n",
    "\n",
    "Otherwise, you can delete the individual resources you created in this tutorial:\n",
    "\n",
    "{TODO: Include commands to delete individual resources below}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up spark session"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-jvidhi-test",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "jvidhi-test on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-jvidhi-test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

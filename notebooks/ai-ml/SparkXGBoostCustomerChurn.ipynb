{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c674ba30-373e-4320-b7b4-d6435988e3f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a32ed9-cf42-4e3e-aa7f-8bc4ae196d63",
   "metadata": {
    "tags": []
   },
   "source": [
    "<table align=\"left\">\n",
    "</td>\n",
    "<td style=\"text-align: center\">\n",
    "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances/create?download_url=https://raw.githubusercontent.com/GoogleCloudDataproc/cloud-dataproc/ai-ml-samples/interactive/ImageClassificationInSpark.ipynb\">\n",
    "<img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "</a>\n",
    "</td>\n",
    "<td style=\"text-align: center\">\n",
    "<a href=\"https://github.com/GoogleCloudDataproc/cloud-dataproc/ai-ml-samples/interactive/ImageClassificationInSpark.ipynb\">\n",
    "<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "</a>\n",
    "</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841dbfd-317c-487a-91a8-cf9613eab108",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, you build a pipeline for predicting customer churn using [Apache Spark](https://spark.apache.org/), [XGBoost](https://xgboost.readthedocs.io/en/latest/index.html), and the [Hugging Face Transformers](https://huggingface.co/docs/transformers/en/index) library. \n",
    "\n",
    "This tutorial is intended to run on [Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs/guides/create-serverless-sessions-templates) on Google Cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff5b074-6611-4e03-bb12-16661b954f18",
   "metadata": {},
   "source": [
    "## Get started\n",
    "\n",
    "1. Create a Dataproc-enabled [Vertex AI workbench instance](https://cloud.google.com/vertex-ai/docs/workbench/instances/create-dataproc-enabled) or use an existing instance.\n",
    "2. Enable [Private Google Access](https://cloud.google.com/dataproc-serverless/docs/concepts/network#private-google-access-requirement) on a subnet in your project.\n",
    "3. Setup [Public NAT](https://cloud.google.com/nat/docs/set-up-manage-network-address-translation#create-nat-gateway) to download models from Hugging Face. See [Dataproc Serverless network configuration](https://cloud.google.com/dataproc-serverless/docs/concepts/network#subnetwork_requirements).\n",
    "4. Create a [serverless runtime template](https://cloud.google.com/dataproc-serverless/docs/quickstarts/jupyterlab-sessions#dataproc_create_serverless_runtime_template-JupyterLab) and connect to a [remote kernel](https://cloud.google.com/vertex-ai/docs/workbench/instances/create-dataproc-enabled#serverless-spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb62046a-da99-454f-a57f-a23bfef62415",
   "metadata": {},
   "source": [
    "Uncomment and run this cell if not using Dataproc Serverless version 1.2 or 2.2+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de803782-3add-4bc8-a321-7bb373c38bfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install xgboost `transformers[torch]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0790e1dc-0838-4f05-a5cb-5fe814d974f0",
   "metadata": {},
   "source": [
    "Import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204fed4-1ad2-4bac-ae19-1882db3f67f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import when, col, regexp_replace, trim\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType, StringType\n",
    "from pyspark.sql.functions import col, isnan, when, count, lit, udf\n",
    "\n",
    "from transformers import pipeline\n",
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196b623f-e326-463a-a592-3433609079ec",
   "metadata": {},
   "source": [
    "Set a name for an existing [Cloud Storage](https://cloud.google.com/storage/docs/creating-buckets) bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdbeb05-806e-438a-9010-3e9bea1b5cca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET = \"YOUR-BUCKET\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a3ee15-27b9-4d45-8a47-41dd0791bff5",
   "metadata": {},
   "source": [
    "Set your project name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fc9ff-b79b-424b-a4c7-8414ca5a23e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"YOUR-PROJECT-ID\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ade501-ee83-4c21-a439-60b911929e6e",
   "metadata": {},
   "source": [
    "# Data transformations and exploratory analysis\n",
    "\n",
    "In this tutorial you use the [IBM Telco Customer Churn](https://github.com/IBM/telco-customer-churn-on-icp4d) dataset. This dataset contains customer demographic and account information for a fictional telecommunications company. You use it to predict which customers are at risk of churn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f579cf36-351b-41ae-8f6f-97b75bde26c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the path to the CSV file\n",
    "file_path = \"https://raw.githubusercontent.com/IBM/telco-customer-churn-on-icp4d/refs/heads/master/data/Telco-Customer-Churn.csv\"\n",
    "\n",
    "# Read the CSV file into a Spark DataFrame. Use pd.read_csv to read directly\n",
    "# from an https link and then convert to Spark.\n",
    "df = spark.createDataFrame(\n",
    "    pd.read_csv(file_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd5a137-47e7-4b66-a977-664dc50a3161",
   "metadata": {},
   "source": [
    "Check for null values in the data, specifically in the `MonthlyCharges` and `TotalCharges` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fcf6e-6f07-4e8b-ae87-7fef5f132409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"MonthlyCharges\", \"TotalCharges\"]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51691326-6eb7-4e06-8f1e-55a581f9f069",
   "metadata": {},
   "source": [
    "Check if `MonthlyCharges` and `TotalCharges` have any misformatted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccd3420-e805-4ee4-8edd-e040e20228ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.select([count(when(col(c).cast(\"float\").isNull(), c)).alias(c) for c in [\"MonthlyCharges\", \"TotalCharges\"]]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace4905c-f704-40ce-a95f-bf85c3074890",
   "metadata": {},
   "source": [
    "Drop null values and cast remaining values to float in TotalCharges to ensure data consistency required for machine learning model training. String and null values can cause errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55346696-18fb-4399-b7f7-f5fa3f3f2003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop records where TotalCharges has null or string values that can't be cast to float\n",
    "df = df.filter(col(\"TotalCharges\").cast(\"float\").isNotNull())\n",
    "\n",
    "# Transform the data type of total_charges to float\n",
    "df = df.withColumn(\"TotalCharges\", col(\"TotalCharges\").cast(FloatType()))\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e37a8d9-a13f-49e5-b877-fa7b6a5c2c17",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visualize the data\n",
    "\n",
    "Visualize churn rates by account types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a2d91-77ae-4bea-9b4d-a8f709219705",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "df_plot_data = df.groupBy(\"Contract\", \"Churn\").count()\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "pdf_plot_data = df_plot_data.toPandas()\n",
    "\n",
    "# Pivot DataFrame to get \"contract\" as index, \"churn\" as columns, and \"count\" as values\n",
    "pdf_pivoted = pdf_plot_data.pivot(index='Contract', columns='Churn', values='count')\n",
    "\n",
    "# Plot\n",
    "pdf_pivoted.plot(kind='bar', stacked=True, figsize=(10, 7), title=\"Customer Count by Contract Type and Churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9cdce9-c54e-4fe2-b843-3fc83f9c6ef2",
   "metadata": {
    "tags": []
   },
   "source": [
    "Next, visualize churn rate by account tenure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628616a0-992f-4b6c-8668-6a3e6b74e179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Aggregate data\n",
    "histogram_data = df.groupBy(\"Tenure\", \"Churn\").count().orderBy(\"Tenure\")\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "pandas_histogram_data = histogram_data.toPandas()\n",
    "\n",
    "# Pivot the data for stacked bar chart\n",
    "pandas_histogram_data_pivot = pandas_histogram_data.pivot(index='Tenure', columns='Churn', values='count').fillna(0)\n",
    "\n",
    "# Plot\n",
    "pandas_histogram_data_pivot.plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    figsize=(12, 7),  # Adjust figure size as needed\n",
    "    title=\"Account Tenure by Churn\",\n",
    "    xlabel=\"Tenure (Months)\",\n",
    "    ylabel=\"Count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f84256-94e9-4040-99e8-a8136a1ce138",
   "metadata": {},
   "source": [
    "## Structured data transformations\n",
    "\n",
    "Convert the `Tenure` field from a monthly bucket to a yearly bucket. This transforms a continuous variable into categorical groups, which can help a model identify non-linear relationships with churn and reduce the impact of outliers in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219ff5ec-58b8-40b3-bddc-55e7e0e0c226",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bucket the \"Tenure\" field due to disribution\n",
    "df = df.withColumn(\"TenureBucket\", \n",
    "    when(col(\"Tenure\") <= 12, \"Tenure_0-12\") \\\n",
    "    .when((col(\"Tenure\") > 12) & (col(\"Tenure\") <= 24), \"Tenure_12-24\") \\\n",
    "    .when((col(\"Tenure\") > 24) & (col(\"Tenure\") <= 48), \"Tenure_24-48\") \\\n",
    "    .when((col(\"Tenure\") > 48) & (col(\"Tenure\") <= 60), \"Tenure_48-60\") \\\n",
    "    .otherwise(\"Tenure_GT_60\"))\n",
    "\n",
    "# Drop the original \"Tenure\" column\n",
    "df = df.drop(\"Tenure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5fa758",
   "metadata": {},
   "source": [
    "Convert boolean columns to integer values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8d140b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform boolean fields to 1 and 0\n",
    "for column in ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']:\n",
    "    df = df.withColumn(column, when(col(column) == True, 1).otherwise(0))\n",
    "\n",
    "# Change \"Churn\" values to 1 and 0\n",
    "df = df.withColumn(\"Churn\", when(col(\"Churn\") == True, 1).otherwise(0)).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bad6d7",
   "metadata": {},
   "source": [
    "Shorten categorical strings to make them easier for the model to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce6a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace \"No internet service\" with \"No\" in specified columns\n",
    "for column in ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']:\n",
    "    df = df.withColumn(column, when(col(column) == \"No internet service\", \"No\").otherwise(col(column)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb6ed0-f457-45d5-b590-8b6041bb352f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Unstructured data transformations\n",
    "\n",
    "Use the Hugging Face `transformers` library to perform sentiment analysis on a series of chat transcripts. Hugging Face provides a library of open source models to select from for a wide variety of use cases.\n",
    "\n",
    "This tutorial uses [DistilBert model](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english). This model performs reasonably well while performing inference quickly. There are other models to choose from that have tradeoffs on performance, speed, accuracy, and model size.\n",
    "\n",
    "This workload will take about 9 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf9370-938b-4368-a300-e2a9064a48b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the model name and revision - downloads from Huggingface\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model_revision = \"af0f99b\" \n",
    "\n",
    "# Load pre-trained sentiment analysis model with specified model and revision\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=model_name, revision=model_revision)\n",
    "\n",
    "# Define a UDF to perform sentiment analysis\n",
    "@udf(returnType=StringType())\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text)[0]\n",
    "        return result['label']\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {text}. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Specify the path to the JSON file in GCS\n",
    "file_path = \"gs://data-analytics-demos/churn/chat_transcripts.json\"\n",
    "\n",
    "# Read the JSON file into a Spark DataFrame\n",
    "df_sentiment = spark.read.json(file_path)\n",
    "\n",
    "# Rename the \"support_transcript\" column to \"text\" (required for this particular model)\n",
    "df_sentiment = df_sentiment.withColumnRenamed(\"support_transcript\", \"text\")\n",
    "\n",
    "# Remove records with null values for text\n",
    "df_sentiment = df_sentiment.filter(df_sentiment[\"text\"].isNotNull() & (df_sentiment[\"text\"] != \"\"))\n",
    "\n",
    "# Analyze the \"support_transcript\" field for sentiment and retain \"customer_id\"\n",
    "df_sentiment = df_sentiment.withColumn(\"sentiment\", analyze_sentiment(df_sentiment[\"text\"])).cache()\n",
    "\n",
    "# Display the results\n",
    "df_sentiment.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943683a-1aa7-4b6d-9a8a-846008b6076c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Join data\n",
    "\n",
    "Combine the transformed transcript data to the customer churn data, then save this to Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e1b691-d216-43bd-9269-55c6a146c774",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join the transformed and sentiment data, and save to GCS\n",
    "\n",
    "# Specify the output path in GCS\n",
    "training_data_path = f\"gs://{BUCKET}/curated/data/training_data\"\n",
    "\n",
    "# Join the DataFrames on customer_id\n",
    "df_combined = df_sentiment.join(df, on=\"CustomerID\", how=\"inner\") \n",
    "\n",
    "# # Write the DataFrame to GCS in Parquet format\n",
    "df_combined.write.format(\"parquet\").mode(\"overwrite\").save(training_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4867478-0467-4e29-8d49-ab0b4e41a197",
   "metadata": {},
   "source": [
    "# Model training and evaluation\n",
    "\n",
    "Next, you train an XGBoost model using the newly processed data. \n",
    "\n",
    "XGBoost does not currently support Spark's [dynamic resource allocation](https://spark.apache.org/docs/3.5.1/job-scheduling.html#dynamic-resource-allocation) feature, a feature of Apache Spark to dynamically allocate additional Spark executors to the environment. As Dataproc Serverless relies on dynamic resource allocation for autoscaling, this means [XGBoost does not autoscale on Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-2.2#xgboost_libraries). The number of executors can be manually configured using the Spark configuration property `spark.executor.instances`.\n",
    "\n",
    "To use XGBoost, disable dynamic allocation first by restarting the Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235b509b-0a8c-40d8-8305-15a0930c4015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "spark = SparkSession.builder.config(\"spark.dynamicAllocation.enabled\", \"false\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346318f-5267-43c4-9153-9fc7fe0e21fd",
   "metadata": {},
   "source": [
    "Load the data again in the new Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a361ece-9bc9-4e66-8c09-f6c6c3373bb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = spark.read.parquet(training_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58137276-e31a-4260-bd86-897c31e01757",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perform feature engineering\n",
    "\n",
    "Convert the data to be usable with SparkML and SparkXGBoost.\n",
    "- String Indexers map the string samples in the data to numbers, which \"shrinks\" the size of the data being processed.\n",
    "- One Hot Encoders convert the categorical data into sparse vectors.\n",
    "- A Vector Assembler is used to format the data per SparkML's requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7c39eb-076d-44d6-874d-e1f727953330",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_cols = ['Partner', 'Dependents', 'PhoneService', 'MultipleLines', \n",
    "                   'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', \n",
    "                   'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', \n",
    "                   'PaperlessBilling', 'PaymentMethod', 'TenureBucket', 'sentiment']\n",
    "numerical_cols = ['SeniorCitizen', 'MonthlyCharges', 'TotalCharges'] \n",
    "\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=c + \"_index\", handleInvalid=\"keep\") for c in categorical_cols]  \n",
    "encoders = [OneHotEncoder(inputCol=c + \"_index\", outputCol=c + \"_encoded\", handleInvalid=\"keep\") for c in categorical_cols] \n",
    "\n",
    "assembler = VectorAssembler(inputCols=[c + \"_encoded\" for c in categorical_cols] + numerical_cols, outputCol=\"features\", handleInvalid=\"keep\") \n",
    "\n",
    "# Apply transformations:\n",
    "indexed_df = Pipeline(stages=indexers).fit(df_train).transform(df_train)\n",
    "encoded_df = Pipeline(stages=encoders).fit(indexed_df).transform(indexed_df)\n",
    "transformed_data = assembler.transform(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc49edcb-1b2f-40f6-b08d-31cc18be7db4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train the model and generate predictions\n",
    "\n",
    "Use the [SparkXGBoostClassifier](https://xgboost.readthedocs.io/en/latest/tutorials/spark_estimator.html#sparkxgbclassifier) to create a SparkML model. This is passed into a SparkML [pipeline](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html) object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bedfb96-f79f-4eb1-a163-b1fc719089de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "xgb = SparkXGBClassifier(label_col=\"Churn\", features_col=\"features\", num_workers=2, missing=0.0)\n",
    "\n",
    "pipeline = Pipeline(stages=[xgb])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dae1ea-b329-4aba-bedc-aed7ff8a1e55",
   "metadata": {
    "tags": []
   },
   "source": [
    "Split the data into training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d648b41-a6a8-4b67-9fce-ca5c4475c790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, test_data = transformed_data.randomSplit([0.8, 0.2], seed=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c993d2a1-6ea2-471d-9289-fe4844e8a624",
   "metadata": {
    "tags": []
   },
   "source": [
    "Fit the model using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec8fe9a-f1f6-490f-acb3-6978621913cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train_data)\n",
    "print(\"Model training successful.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d4818-d509-4089-961f-eb98377eefa8",
   "metadata": {},
   "source": [
    "Generate predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98238a24-3545-4fea-acc8-09d696c73d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c094288c-8f29-463b-9634-e5738c6d2188",
   "metadata": {},
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Evaluate the performance of the model. These metrics provide a means to determine how effective a model is at predicting outcomes on new data. It is useful to track these metrics overtime. The metrics used are:\n",
    "- [AUC](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "- [Accuracy](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall#accuracy)\n",
    "- [F1 score](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall#expandable-2)\n",
    "- [Precision](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall#precision)\n",
    "- [Recall](https://developers.google.com/machine-learning/crash-course/classification/accuracy-precision-recall#recall_or_true_positive_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0621ab-73b5-4a01-9fa2-e8b8e7d7bbfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluator_binary = BinaryClassificationEvaluator(labelCol=\"Churn\", rawPredictionCol=\"rawPrediction\")\n",
    "auc = evaluator_binary.evaluate(predictions, {evaluator_binary.metricName: \"areaUnderROC\"})\n",
    "print(f\"Area Under ROC (AUC): {auc:.4f}\")\n",
    "\n",
    "evaluator_multi = MulticlassClassificationEvaluator(labelCol=\"Churn\", predictionCol=\"prediction\")\n",
    "accuracy = evaluator_multi.evaluate(predictions, {evaluator_multi.metricName: \"accuracy\"})\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "f1 = evaluator_multi.evaluate(predictions, {evaluator_multi.metricName: \"f1\"})\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "weighted_precision = evaluator_multi.evaluate(predictions, {evaluator_multi.metricName: \"weightedPrecision\"})\n",
    "print(f\"Weighted Precision: {weighted_precision:.4f}\")\n",
    "\n",
    "weighted_recall = evaluator_multi.evaluate(predictions, {evaluator_multi.metricName: \"weightedRecall\"})\n",
    "print(f\"Weighted Recall: {weighted_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3735271-a6b3-4077-b80e-c362eb22cacf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save to Cloud Storage\n",
    "\n",
    "Save the train data, test data, and XGBoost model to Cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d3964a-4314-4074-a673-efd1d7016e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify GCS bucket training data path\n",
    "data_path = f\"gs://{BUCKET}/curated/models/churn_prediction\"\n",
    "\n",
    "# Save transformed training data\n",
    "train_data.write.parquet(f\"{data_path}/train_data\", mode=\"overwrite\")\n",
    "\n",
    "# Save transformed test data\n",
    "test_data.write.parquet(f\"{data_path}/test_data\", mode=\"overwrite\")\n",
    "\n",
    "# Save the trained model to GCS:\n",
    "model.write().overwrite().save(f\"{data_path}/xgboost_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceb3128-cef5-496b-ba94-bc3c3c7622c0",
   "metadata": {},
   "source": [
    "# Run batch inference on Spark\n",
    "\n",
    "Load the model and test data from Cloud Storage into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e4f56d-494d-434a-8540-07d720582a9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify my GCP bucket\n",
    "model_bucket = f\"gs://{BUCKET}/curated/models/churn_prediction\"\n",
    "\n",
    "# Load transformed training data\n",
    "test_data_loaded = spark.read.parquet(f\"{model_bucket}/test_data\")\n",
    "\n",
    "# Load the trained model\n",
    "model_loaded = PipelineModel.load(f\"{model_bucket}/xgboost_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828831e8-e043-459f-a68a-19e5e0377151",
   "metadata": {},
   "source": [
    "Generate predictions on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25957ca9-1170-405e-b5a0-7f6123fae29f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = model_loaded.transform(test_data_loaded)\n",
    "\n",
    "# Filter down to only include the customer_id and prediction columns\n",
    "predictions = predictions.select(\"customerID\", \"prediction\", \"rawPrediction\", \"probability\")\n",
    "\n",
    "# Display sample data\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd2a047-3e3a-420e-b7d5-6374aa9f6e3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save predictions to BigQuery\n",
    "\n",
    "Save the data to BigQuery. While out of scope for this tutorial, saving to BigQuery enables additional use cases such as joining this data with other BigQuery data or creating visualizations using a tool such as [Looker](cloud.google.com/looker).\n",
    "\n",
    "Create a BigQuery dataset to store your predictions in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e0d053-19cc-4fcb-9736-c5e4588c1ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Construct a BigQuery client object.\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "# Set dataset_id to the ID of the dataset to create.\n",
    "dataset_id = f\"{PROJECT_ID}.predictions\"\n",
    "\n",
    "# Construct a full Dataset object to send to the API.\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "\n",
    "# TODO(developer): Specify the geographic location where the dataset should reside.\n",
    "dataset.location = \"us-central1\"\n",
    "\n",
    "# Send the dataset to the API for creation, with an explicit timeout.\n",
    "dataset = bigquery_client.create_dataset(dataset, timeout=30)  # Make an API request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ff34de-baed-4ee5-ae24-57b850235028",
   "metadata": {},
   "source": [
    "Save the predictions directly from Spark to BigQuery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da6e2e1-4b51-49b6-898b-e64e05a4346a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define table name to save predictions\n",
    "table_name = 'predictions'\n",
    "\n",
    "# Write predictions DataFrame to table\n",
    "predictions.write \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"writeMethod\", \"direct\") \\\n",
    "    .save(f'{dataset_id}.{table_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca8bd4e",
   "metadata": {},
   "source": [
    "# Clean up\n",
    "\n",
    "[Delete](https://cloud.google.com/resource-manager/docs/creating-managing-projects#shutting_down_projects) your project or use the following steps to delete project resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aaea0cc-e634-40db-aa3b-5321b63f9e1f",
   "metadata": {},
   "source": [
    "Uncomment to delete the BigQuery dataset and table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a5f5f-e3d1-4130-b58c-00a98f71fae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bigquery_client.delete_dataset(dataset_id, delete_contents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6850561d-f3f9-4471-9751-742dafbf2882",
   "metadata": {
    "tags": []
   },
   "source": [
    "Uncomment to delete either the Cloud Storage bucket or only the content this tutorial created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d217b44-4105-48cf-b570-fe237a0bdc0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(BUCKET)\n",
    "\n",
    "# Uncomment to DELETE ONLY THE CONTENTS created in this tutorial.\n",
    "# for blob in bucket.list_blobs(prefix=\"curated\"):\n",
    "#     blob.delete()\n",
    "\n",
    "# Uncomment to DELETE THE BUCKET.\n",
    "# bucket.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3a577-4f0d-40ee-9915-d0408d8d7873",
   "metadata": {},
   "source": [
    "To delete the Vertex AI Workbench instance, go to the [Instances](https://console.cloud.google.com/vertex-ai/workbench/instances) page, select your instance and click **DELETE**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70138d0-256f-4508-b2dd-9ac5c7298e9d",
   "metadata": {},
   "source": [
    "To delete the Dataproc Serverless interactive session, go to the [Interactive Sessions](https://console.cloud.google.com/dataproc/interactive) page, click your session and click **DELETE**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ddefd7-2700-4975-ac3d-5a94b9373f09",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "- [Try more tutorials using Spark and Dataproc Serverless for machine learning](https://github.com/GoogleCloudDataproc/cloud-dataproc/tree/master/notebooks/ai-ml)\n",
    "- [Learn more about Dataproc Serverless](https://cloud.google.com/dataproc-serverless/docs)\n",
    "- [Learn more about Vertex AI](https://cloud.google.com/vertex-ai)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-runtime-0000a91841a7",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "121124 on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-runtime-0000a91841a7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

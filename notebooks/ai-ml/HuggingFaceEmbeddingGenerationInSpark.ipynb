{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Generate Text Embeddings with Hugging face model using Apache Spark on Vertex Workbench"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "The example creates a similarity search on Stackoverflow questions to identify similar topics, questions and technologies being discussed. It leverages BigQuery and Dataproc Serverless for distributed prediction on Deep Learning models.\n",
    "\n",
    "Data Engineers and Data Scientists with existing working knowledge of BigQuery and Dataproc/Spark can use this notebook to launch batch inference jobs at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn how to use Apache Spark for batch inference/prediction and BQ for Vector Search. You also learn to use Dataproc Interactive Sessions from Jupyter Notebooks - From Vertex Workbench Instance\n",
    "\n",
    "The example uses open source stackoverflow data and open source Hugging Face model - all-MiniLM-L12-v2 to generate embeddings of text data. https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2 The model maps text data into 384 dimensional dense vector space. The similarity search on vector index is created in BigQuery.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services and resources:\n",
    "\n",
    "- BQML - Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08d289fa873f"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "BigQuery public dataset - \"bigquery-public-data.stackoverflow\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aed92deeb4a0"
   },
   "source": [
    "### Costs \n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Dataproc Serverless\n",
    "* BigQuery\n",
    "* Vertex Workbench Instance / BQ Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "[BigQuery pricing](https://cloud.google.com/bigquery/pricing),\n",
    "and [Dataproc Serverless Pricing](https://cloud.google.com/dataproc-serverless/pricing), \n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BF1j6f9HApxa",
    "tags": []
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
    "[Enable the Dataproc API](https://console.cloud.google.com/flows/enableapi?apiid=dataproc.googleapis.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7EUnXsZhAGF"
   },
   "source": [
    "## Setup & Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create Dataproc Interactive Session Template with Autoscaling\n",
    "\n",
    "Create a [Dataproc Interactive Session Template](https://cloud.google.com/dataproc-serverless/docs/guides/create-serverless-sessions-templates) using the network configuration specified in the link.\n",
    "\n",
    "Enable Autoscaling - https://cloud.google.com/dataproc-serverless/docs/concepts/autoscaling\n",
    "\n",
    "Required Parameters to enable Autoscaling :\n",
    "\n",
    "    spark.dynamicAllocation.enabled = true\n",
    "    spark.dynamicAllocation.maxExecutors = 100\n",
    "    spark.dynamicAllocation.minExecutors = 5\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init_aip:mbsdk,all",
    "tags": []
   },
   "source": [
    "#### Select Dataproc Serverless Interactive Session as the Kernel for this notebook\n",
    "\n",
    "Once the Template is created, select the interactive template as the kernel for the notebook. This will create Dataproc Interactive Session [check here](https://console.cloud.google.com/dataproc/interactive?)\n",
    "\n",
    "This may take a while, so please dont close the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark runtime 2.2 has many ML libraries pre-installed. Check - https://cloud.google.com/dataproc-serverless/docs/concepts/versions/spark-runtime-2.2 before re-installing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to certain dependencies between Hugging Face models, we ensure if the numpy version is 1.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "oM1iC_MfAts1"
   },
   "outputs": [],
   "source": [
    "project_id = '[your-project-id]'  # @param {type:\"string\"}\n",
    "region = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from pyspark.ml.functions import predict_batch_udf\n",
    "from pyspark.sql.functions import struct, col, array, udf, lit, split\n",
    "from pyspark.sql.types import ArrayType, FloatType, Union, Dict\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a BigQuery Dataset - https://cloud.google.com/bigquery/docs/datasets#console "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bq_dataset = '[your-bigquery-dataset]'\n",
    "table_name = f'{bq_dataset}.stackoverflow_questions'\n",
    "index = f'{bq_dataset}.stackoverflow_index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session & load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/02 09:21:18 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Embeddings\")\\\n",
    ".config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.13:3.5.1\") \\\n",
    ".getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = spark.read.format('bigquery') \\\n",
    "  .option('table', 'bigquery-public-data.stackoverflow.posts_questions') \\\n",
    "  .load()\n",
    "\n",
    "df_data = df_data.select('title', 'tags').filter(col(\"title\").isNotNull())\n",
    "\n",
    "# To remove special characters from title for text embedding model\n",
    "df_data = df_data.withColumn('title_mod', regexp_replace(\"title\", '[^A-Za-z0-9.,]',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=================================================>       (50 + 8) / 58]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|               title|                tags|           title_mod|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Which internal fo...|   3d-texture|webgl2|Which internal fo...|\n",
      "|How to scroll to ...|jquery|mousewheel...|How to scroll to ...|\n",
      "|How To read Colum...|          apache-poi|How To read Colum...|\n",
      "|cudaMemcpy2D erro...|            c++|cuda|cudaMemcpy2D erro...|\n",
      "|Query SQL on Orie...|            orientdb|Query SQL on Orie...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['title', 'tags', 'title_mod']\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(df_data.columns)\n",
    "print(df_data.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create batch prediction function\n",
    "\n",
    "The model will be called and loaded within the batch predict function which will load the model in executors and run distributed inference on spark dataframe\n",
    "Learn more - https://spark.apache.org/docs/3.4.3/api/python/reference/api/pyspark.ml.functions.predict_batch_udf.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch_fn():\n",
    "    import torch\n",
    "    from pyspark.sql.types import ArrayType, StringType\n",
    "    \n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Using {} device\".format(device))\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "    model.to(device)\n",
    "    \n",
    "    def predict(inputs: ArrayType(StringType())) -> np.ndarray:\n",
    "        embeddings = model.encode(inputs) #size [batch_size]\n",
    "        return embeddings #return (batch_size,384)\n",
    "    \n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embeddings_udf = predict_batch_udf(predict_batch_fn,\n",
    "                          return_type=ArrayType(FloatType()),\n",
    "                          batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.83 ms, sys: 5.6 ms, total: 14.4 ms\n",
      "Wall time: 79.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "prediction = df_data.withColumn(\"embeddings\", generate_embeddings_udf('title_mod'))\n",
    "prediction = prediction.withColumn(\"array_size\", size(prediction[\"embeddings\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- tags: string (nullable = true)\n",
      " |-- title_mod: string (nullable = true)\n",
      " |-- embeddings: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- array_size: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validating the length of embeddings generated [384] as part of data validation step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import countDistinct, count\n",
    "\n",
    "prediction_filtered = prediction.filter(size(prediction[\"embeddings\"]) == 384)\n",
    "df_grouped = prediction_filtered.groupBy(\"array_size\").agg(count(\"array_size\").alias(\"row_count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the dataframe as a table in BigQuery. We will create a vector index on this table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "prediction.write\\\n",
    ".mode('overwrite')\\\n",
    ".format(\"bigquery\")\\\n",
    ".option(\"writeMethod\",\"direct\")\\\n",
    ".option(\"useAvroLogicalTypes\", \"true\")\\\n",
    ".option(\"table\",f{table_name}\"_embeddings\")\\\n",
    ".save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vector index in BigQuery \n",
    "https://cloud.google.com/bigquery/docs/vector-index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "client.dataset(bq_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand variables in BigQuery Magic commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9336cb00bf564cac99f3dca042364a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Query is running:   0%|          |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery\n",
    "CREATE VECTOR INDEX <bq_dataset>.<index> ON <bq_dataset>.<table_name>_embeddings(embeddings)\n",
    "OPTIONS (index_type = 'TREE_AH', distance_type = 'EUCLIDEAN',\n",
    "tree_ah_options = '{\"normalization_type\": \"L2\"}');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generate embedding of the search query which will be searched on the vector index to find similar search items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_sentence = \"Hadoop and Apache Spark\"\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L12-v2')\n",
    "embeddings = model.encode(query_sentence).tolist()\n",
    "# embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT * FROM\n",
    "      VECTOR_SEARCH ( TABLE @table,\n",
    "      'embeddings',\n",
    "      (select @embeddings),\n",
    "        top_k => 5, options => '{\"fraction_lists_to_search\": 0.01}');\n",
    "\"\"\"\n",
    "job_config = bigquery.QueryJobConfig(\n",
    "    query_parameters=[\n",
    "        bigquery.ScalarQueryParameter(\"table\", \"STRING\", f'{table_name}_embeddings'),\n",
    "        bigquery.ArrayQueryParameter(\"embeddings\", \"FLOAT\", embeddings),\n",
    "    ]\n",
    ")\n",
    "query_job = client.query(query, job_config=job_config)  # Make an API request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing HDFS commands in hadoop\n",
      "Distance: 0.7820840860574579\n",
      "\n",
      "Direct HDFS access in hadooponazure\n",
      "Distance: 0.8784149879587282\n",
      "\n",
      "using chmod intalling hadoop\n",
      "Distance: 0.9152823934170262\n",
      "\n",
      "Using Hadoop & related projects to analyze usage patterns that constantly change\n",
      "Distance: 0.9157446105036288\n",
      "\n",
      "Performance-wise, is it better to use Hadoop over MPI for a typical embarrassingly parallel scenario?\n",
      "Distance: 0.9221166140003834\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in query_job:\n",
    "    print(row[1]['title'])\n",
    "    print(f'Distance: {row[2]}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpV-iwP9qw9c"
   },
   "source": [
    "## Cleaning up\n",
    "\n",
    "To clean up all Google Cloud resources used in this project, you can delete BQ Dataset you used for the tutorial.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "9c39b79e5d2e7072beb4bd59-jvidhi-prebuilt-infer",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "jvidhi-prebuilt-infer on Serverless Spark (Remote)",
   "language": "python",
   "name": "9c39b79e5d2e7072beb4bd59-jvidhi-prebuilt-infer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

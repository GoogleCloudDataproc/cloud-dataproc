{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<table align=\"left\">\n",
    "</td>\n",
    "<td style=\"text-align: center\">\n",
    "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/instances/create?download_url=https://raw.githubusercontent.com/GoogleCloudDataproc/cloud-dataproc/ai-ml-samples/interactive/ImageClassificationInSpark.ipynb\">\n",
    "<img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "</a>\n",
    "</td>\n",
    "<td style=\"text-align: center\">\n",
    "<a href=\"https://github.com/GoogleCloudDataproc/cloud-dataproc/ai-ml-samples/interactive/ImageClassificationInSpark.ipynb\">\n",
    "<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "</a>\n",
    "</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "In this tutorial, you perform distributed ML inference via image classification using Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Get started\n",
    "\n",
    "1. Create a dataproc-enabled [Vertex workbench](https://cloud.google.com/vertex-ai/docs/workbench/instances/create-dataproc-enabled) instance or use an existing instance.\n",
    "2. Enable [Private Google Access](https://cloud.google.com/dataproc-serverless/docs/concepts/network#private-google-access-requirement) on a subnet in your project.\n",
    "2. Setup [Public NAT](https://cloud.google.com/nat/docs/set-up-manage-network-address-translation#create-nat-gateway to download Torch model weights. See [\"External network access\"](https://cloud.google.com/dataproc-serverless/docs/concepts/network#subnetwork_requirements).\n",
    "3. Create a [serverless runtime template](https://cloud.google.com/dataproc-serverless/docs/quickstarts/jupyterlab-sessions#dataproc_create_serverless_runtime_template-JupyterLab) and connect to a [remote kernel](https://cloud.google.com/vertex-ai/docs/workbench/instances/create-dataproc-enabled#serverless-spark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5303c05f7aa6"
   },
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All libraries needed in this notebook are installed in Dataproc Serverless versions 1.2 and 2.2+."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell if not using Dataproc Serverless version 1.2 or 2.2+.\n",
    "# pip install torch torchvision google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Google Cloud Storage client library and helper functions\n",
    "from google.cloud import storage\n",
    "from google.cloud.storage.blob import Blob\n",
    "\n",
    "# Import Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import PySpark helper functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "# Import Pytorch and helper functions\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import  transforms\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e43229f3ad4f"
   },
   "source": [
    "Get the list of URIs for the images to be classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cf93d5f0ce00"
   },
   "outputs": [],
   "source": [
    "# Set the bucket name and number of images to classify. Feel free to experiment with more or less images.\n",
    "bucket_name = 'cloud-samples-data'\n",
    "max_results = 50\n",
    "\n",
    "# Load the bucket \n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Get the blob URI\n",
    "blobs = bucket.list_blobs(prefix=\"generative-ai/image\", max_results=max_results)\n",
    "blob_uris = []\n",
    "for blob in blobs:\n",
    "    (blob_uris.append(f\"gs://{bucket_name}/{blob.name}\") \n",
    "    if blob.name.endswith(\"jpg\") else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a4e033321ad"
   },
   "source": [
    "Create a Spark session and load the data into Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a SparkSession object.\n",
    "spark = SparkSession.builder.appName(\"classificationDemo\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Enable Apache Arrow support.\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"64\")\n",
    "\n",
    "# Create the Spark dataframe.\n",
    "files_df = spark.createDataFrame(blob_uris, StringType()) \\\n",
    "                .withColumnRenamed(\"value\", \"inputFile\") \\\n",
    "                .repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a [custom Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files) object to manage the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, paths, transform=None):\n",
    "        self.paths = paths\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        client = storage.Client()\n",
    "        path = self.paths[index]\n",
    "        # Download file from GCS as image loader needs local file\n",
    "        blob = Blob.from_string(path, client=client)\n",
    "        local_file = \"/tmp/\" + path.split(\"/\")[-1]\n",
    "        blob.download_to_file(open(local_file, \"wb\"))\n",
    "        image = default_loader(local_file)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Create a [Pandas UDF](https://spark.apache.org/docs/3.4.2/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html) that contains the Torch code that will run on each worker to perform model training. This training job uses the [ResNet50](https://spark.apache.org/docs/3.4.2/api/python/reference/pyspark.sql/api/pyspark.sql.functions.pandas_udf.html) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@pandas_udf(ArrayType(FloatType()))\n",
    "def predict_batch_udf(paths: pd.Series) -> pd.Series:\n",
    "\n",
    "  #Transformation needed on input by Resnet model\n",
    "  transform = transforms.Compose([\n",
    "      transforms.Resize(224),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "    \n",
    "  # Create image dataset\n",
    "  images = ImageDataset(paths, transform=transform)\n",
    "    \n",
    "  # Tune batch_size/num_workers based on your workload\n",
    "  loader = torch.utils.data.DataLoader(images, batch_size=500, num_workers=8)\n",
    "    \n",
    "  # Set local directory to hold models\n",
    "  torch.hub.set_dir(\"/tmp/models\")\n",
    "    \n",
    "  # Configure if jobs will run on CPU or GPU\n",
    "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "  \n",
    "  # Initialize model and load onto device\n",
    "  model = resnet50(ResNet50_Weights.DEFAULT)\n",
    "  model.to(device)\n",
    "  \n",
    "  # Run predictions\n",
    "  all_predictions = []\n",
    "  with torch.no_grad():\n",
    "    for batch in loader:\n",
    "      predictions = list(model(batch.to(device)).numpy())\n",
    "      for prediction in predictions:\n",
    "        all_predictions.append(prediction)\n",
    "  return pd.Series(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Execute the training job and convert the output to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_df = files_df.select(\n",
    "    col(\"inputFile\"),\n",
    "    predict_batch_udf(col(\"inputFile\")).alias(\"predictions\"))\n",
    "predictions = predictions_df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Get labels based on the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = ResNet50_Weights.DEFAULT\n",
    "predictions[\"label\"] = predictions[\"predictions\"].map(lambda x: weights.meta[\"categories\"][x.argmax()])\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "predictions.head(10)[[\"inputFile\", \"label\"]]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "notebook_template.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1. BigQuery Storage & Spark DataFrame - Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python 3 Kernel\n",
    "\n",
    "Use a Python 3 kernel (not PySpark) to allow you to configure the SparkSession in the notebook and include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) required to use the [BigQuery Storage API](https://cloud.google.com/bigquery/docs/reference/storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Version\n",
    "\n",
    "Check what version of Scala you are running so you can include the correct spark-bigquery-connector jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session\n",
    "\n",
    "Include the correct version of the spark-bigquery-connector jar\n",
    "\n",
    "Scala version 2.11 - `'gs://spark-lib/bigquery/spark-bigquery-latest.jar'`.\n",
    "\n",
    "Scala version 2.12 - `'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName('BigQuery Storage & Spark DataFrame - Python')\\\n",
    "  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read BigQuery table into Spark DataFrame\n",
    "\n",
    "Use `filter()` to query data from a partitioned table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datehour: timestamp (nullable = true)\n",
      " |-- wiki: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table = \"bigquery-public-data.wikipedia.pageviews_2020\"\n",
    "df = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\", table) \\\n",
    "  .option(\"filter\", \"datehour >= '2020-03-01' AND datehour < '2020-03-02'\") \\\n",
    "  .load()\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select required columns and apply a filter using `where()` which is an alias for `filter()` then cache the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-----+\n",
      "|               title|wiki|views|\n",
      "+--------------------+----+-----+\n",
      "|2020_Democratic_P...|  en| 3242|\n",
      "|Eurovision_Song_C...|  en| 2368|\n",
      "|         Colin_McRae|  en| 2360|\n",
      "|        Donald_trump|  en| 2223|\n",
      "|Comparison_of_onl...|  en| 1398|\n",
      "+--------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df \\\n",
    "  .select(\"title\", \"wiki\", \"views\") \\\n",
    "  .where(\"views > 1000 AND wiki in ('en', 'en.m')\") \\\n",
    "  .cache()\n",
    "\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group by title and find top 20 pages by page views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|               title|total_views|\n",
      "+--------------------+-----------+\n",
      "|           Main_Page|   10939337|\n",
      "|United_States_Senate|    5619797|\n",
      "|                   -|    3852360|\n",
      "|      Special:Search|    1538334|\n",
      "|2019–20_coronavir...|     407042|\n",
      "|2020_Democratic_P...|     260093|\n",
      "|         Coronavirus|     254861|\n",
      "|The_Invisible_Man...|     233718|\n",
      "|       Super_Tuesday|     201077|\n",
      "|         Colin_McRae|     200219|\n",
      "|         David_Byrne|     189989|\n",
      "|2019–20_coronavir...|     156803|\n",
      "|        John_Mulaney|     155605|\n",
      "|2020_South_Caroli...|     152137|\n",
      "|      AEW_Revolution|     140503|\n",
      "|       Boris_Johnson|     120957|\n",
      "|          Tom_Steyer|     120926|\n",
      "|Dyatlov_Pass_inci...|     117704|\n",
      "|         Spanish_flu|     108335|\n",
      "|2020_coronavirus_...|     107653|\n",
      "+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df2 = df1 \\\n",
    ".groupBy(\"title\") \\\n",
    ".agg(F.sum('views').alias('total_views'))\n",
    "\n",
    "df2.orderBy('total_views', ascending=False).show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write Spark Dataframe to BigQuery table\n",
    "\n",
    "Write the Spark Dataframe to BigQuery table using BigQuery Storage connector. This will also create the table if it does not exist. The GCS bucket and BigQuery dataset must already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'dataproc-bucket-name'\n",
    "\n",
    "df2.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\",\"dataset_name.wiki_total_pageviews\") \\\n",
    "  .option(\"temporaryGcsBucket\", bucket_name) \\\n",
    "  .mode('overwrite') \\\n",
    "  .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
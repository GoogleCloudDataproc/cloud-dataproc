{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3. BigQuery Storage & Spark ML - Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python 3 Kernel\n",
    "\n",
    "Use a Python 3 kernel (not PySpark) to allow you to configure the SparkSession in the notebook and include the [spark-bigquery-connector](https://github.com/GoogleCloudDataproc/spark-bigquery-connector) required to use the [BigQuery Storage API](https://cloud.google.com/bigquery/docs/reference/storage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scala Version\n",
    "\n",
    "Check what version of Scala you are running so you can include the correct spark-bigquery-connector jar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat: /release: No such file or directory\n",
      "Scala code runner version 2.11.12 -- Copyright 2002-2017, LAMP/EPFL\n"
     ]
    }
   ],
   "source": [
    "!scala -version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session\n",
    "\n",
    "Include the correct version of the spark-bigquery-connector jar\n",
    "\n",
    "If you are using scala version 2.11 use `'gs://spark-lib/bigquery/spark-bigquery-latest.jar'`.\n",
    "\n",
    "If you are using scala version 2.12 use `'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "  .appName('BigQuery Storage &  Spark ML')\\\n",
    "  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest.jar') \\\n",
    "  .getOrCreate()\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from BigQuery as a Spark Dataframe.\n",
    "table  = 'bigquery-public-data.samples.natality'\n",
    "\n",
    "df_natality_table = spark.read \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\", table) \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- weight_pounds: double (nullable = true)\n",
      " |-- mother_age: long (nullable = true)\n",
      " |-- father_age: long (nullable = true)\n",
      " |-- gestation_weeks: long (nullable = true)\n",
      " |-- weight_gain_pounds: long (nullable = true)\n",
      " |-- apgar_5min: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# limit no of rows that will be read for demo \n",
    "limit = 1000\n",
    "\n",
    "df_natality_select = df_natality_table \\\n",
    ".select(\"weight_pounds\", \"mother_age\", \"father_age\", \"gestation_weeks\", \"weight_gain_pounds\", \"apgar_5min\") \\\n",
    ".where(\"\"\"\n",
    "weight_pounds IS NOT NULL \n",
    "AND mother_age IS NOT NULL\n",
    "AND father_age IS NOT NULL\n",
    "AND gestation_weeks IS NOT NULL\n",
    "AND weight_gain_pounds IS NOT NULL\n",
    "AND apgar_5min IS NOT NULL\n",
    "\"\"\") \\\n",
    ".limit(limit) \\\n",
    ".cache()\n",
    "\n",
    "df_natality_select.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_natality_select.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# As a precaution, run a query in Spark SQL to ensure no NULL values exist.\n",
    "\n",
    "# Create a view so that Spark SQL queries can be run against the data.\n",
    "df_natality_select.createOrReplaceTempView(\"natality\")\n",
    "spark_sql_query = \"\"\"\n",
    "SELECT *\n",
    "from natality\n",
    "where weight_pounds is not null\n",
    "and mother_age is not null\n",
    "and father_age is not null\n",
    "and gestation_weeks is not null\n",
    "\"\"\"\n",
    "\n",
    "df_natality_select = spark.sql(spark_sql_query).cache()\n",
    "df_natality_select.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------------+------------------+----------+--------------------+\n",
      "|     weight_pounds|mother_age|father_age|gestation_weeks|weight_gain_pounds|apgar_5min|            features|\n",
      "+------------------+----------+----------+---------------+------------------+----------+--------------------+\n",
      "|     8.62889293468|        34|        38|             41|                57|         9|[34.0,38.0,41.0,5...|\n",
      "|      2.6786164833|        36|        39|             34|                23|         6|[36.0,39.0,34.0,2...|\n",
      "|    11.06279630716|        38|        41|             41|                11|         9|[38.0,41.0,41.0,1...|\n",
      "|     5.43659938092|        42|        42|             38|                10|         9|[42.0,42.0,38.0,1...|\n",
      "|3.5604655312999998|        38|        43|             31|                18|         8|[38.0,43.0,31.0,1...|\n",
      "|     5.99877814902|        37|        42|             39|                20|         9|[37.0,42.0,39.0,2...|\n",
      "|     9.18666245754|        28|        36|             38|                37|         8|[28.0,36.0,38.0,3...|\n",
      "|     4.87442061282|        26|        27|             36|                17|         9|[26.0,27.0,36.0,1...|\n",
      "| 8.000575487979999|        36|        39|             40|                13|         9|[36.0,39.0,40.0,1...|\n",
      "|      6.5256829552|        33|        34|             39|                99|        99|[33.0,34.0,39.0,9...|\n",
      "|      6.5697754076|        39|        37|             37|                99|        99|[39.0,37.0,37.0,9...|\n",
      "|      7.6610636045|        25|        31|             41|                99|        99|[25.0,31.0,41.0,9...|\n",
      "|     5.43659938092|        29|        35|             33|                99|        99|[29.0,35.0,33.0,9...|\n",
      "|     8.50102482272|        40|        61|             40|                99|        99|[40.0,61.0,40.0,9...|\n",
      "| 8.375361333379999|        39|        40|             39|                69|         9|[39.0,40.0,39.0,6...|\n",
      "|     4.87442061282|        29|        34|             34|                45|         9|[29.0,34.0,34.0,4...|\n",
      "|     5.00008410216|        36|        39|             33|                32|         8|[36.0,39.0,33.0,3...|\n",
      "| 8.811876612139999|        41|        99|             40|                18|         8|[41.0,99.0,40.0,1...|\n",
      "|     8.24969784404|        35|        39|             39|                16|        99|[35.0,39.0,39.0,1...|\n",
      "|      6.8122838958|        33|        40|             38|                10|        99|[33.0,40.0,38.0,1...|\n",
      "+------------------+----------+----------+---------------+------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create an input DataFrame for Spark ML using VectorAssembler.\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"mother_age\", \"father_age\", \"gestation_weeks\", \"weight_gain_pounds\", \"apgar_5min\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(df_natality_select)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|            features|             label|\n",
      "+--------------------+------------------+\n",
      "|[34.0,38.0,41.0,5...|     8.62889293468|\n",
      "|[36.0,39.0,34.0,2...|      2.6786164833|\n",
      "|[38.0,41.0,41.0,1...|    11.06279630716|\n",
      "|[42.0,42.0,38.0,1...|     5.43659938092|\n",
      "|[38.0,43.0,31.0,1...|3.5604655312999998|\n",
      "|[37.0,42.0,39.0,2...|     5.99877814902|\n",
      "|[28.0,36.0,38.0,3...|     9.18666245754|\n",
      "|[26.0,27.0,36.0,1...|     4.87442061282|\n",
      "|[36.0,39.0,40.0,1...| 8.000575487979999|\n",
      "|[33.0,34.0,39.0,9...|      6.5256829552|\n",
      "|[39.0,37.0,37.0,9...|      6.5697754076|\n",
      "|[25.0,31.0,41.0,9...|      7.6610636045|\n",
      "|[29.0,35.0,33.0,9...|     5.43659938092|\n",
      "|[40.0,61.0,40.0,9...|     8.50102482272|\n",
      "|[39.0,40.0,39.0,6...| 8.375361333379999|\n",
      "|[29.0,34.0,34.0,4...|     4.87442061282|\n",
      "|[36.0,39.0,33.0,3...|     5.00008410216|\n",
      "|[41.0,99.0,40.0,1...| 8.811876612139999|\n",
      "|[35.0,39.0,39.0,1...|     8.24969784404|\n",
      "|[33.0,40.0,38.0,1...|      6.8122838958|\n",
      "+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training_data = output.select(\"features\", \"weight_pounds\").withColumnRenamed(\"weight_pounds\",\"label\")\n",
    "training_data.cache()\n",
    "training_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Construct a new LinearRegression object and fit the training data.\n",
    "lr = LinearRegression(maxIter=5, regParam=0.2, solver=\"normal\")\n",
    "model = lr.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:[0.03210580452462181,-0.005448902107015862,0.20940221495236297,0.0015140425323880477,0.001831082053035294]\n",
      "Intercept:-1.5655754043662446\n",
      "R^2:0.24565216749816532\n",
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|  0.6216582894053886|\n",
      "|  -3.864594671990001|\n",
      "|  3.0131311065977986|\n",
      "| -2.1063194482441956|\n",
      "|  -2.393046931198222|\n",
      "| -1.6081542977973324|\n",
      "|  2.0214834127569716|\n",
      "| -1.8283327433774765|\n",
      "| 0.21059822214059487|\n",
      "| -1.2808242352068033|\n",
      "| -0.9942154737287598|\n",
      "|-0.32374828593560157|\n",
      "| -0.9796223995671225|\n",
      "|  0.4074951425779094|\n",
      "|  0.6191313892123773|\n",
      "|   -1.51009660320437|\n",
      "| -1.3510133850752002|\n",
      "|   1.182565319489532|\n",
      "|    0.53188908530724|\n",
      "| -0.6173778816298086|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the model summary.\n",
    "print(\"Coefficients:\" + str(model.coefficients))\n",
    "print(\"Intercept:\" + str(model.intercept))\n",
    "print(\"R^2:\" + str(model.summary.r2))\n",
    "model.summary.residuals.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
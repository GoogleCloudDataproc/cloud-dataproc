{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# BigQuery Read/Write with GDS, Spark Connector and PySpark\n",
    "In the examples that follows, we will be using the Spark Connector running under PySpark\n",
    "[Neo4j spark connector under Python](https://neo4j.com/docs/spark/current/python/) and the Graph Data Science (GDS) client.\n",
    "\n",
    "Please run this notebook from a valid Spark environment.  It was tested under [DataProc](https://cloud.google.com/dataproc)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j instance\n",
    "Create a free account at https://sandbox.neo4j.com. Choose the “Blank Sandbox - Graph Data Science” option.\n",
    "When your sandbox has been created, fill in the Bolt URL and password below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup Neo4j Spark Connector imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from graphdatascience import GraphDataScience\n",
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "from pyspark import SparkFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Define Neo4j connection variables.  Yours will be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4j_url = \"bolt://***removed***:7687\"\n",
    "neo4j_user = \"neo4j\"\n",
    "neo4j_password = \"***removed***\"\n",
    "neo4j_database= \"neo4j\"\n",
    "tmp_working_bucket = \"neo4j-sandbox/dataproc-working\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create Spark Session, seeded with Neo4j packages.  If you don't want to wait for the download each time, load the connector into the master node using SSH.\n",
    "\n",
    "Note: we are adding the Neo4j data warehouse connector and BigQuery library in the library packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder\n",
    "        .appName('Leverage Neo4j with Apache Spark')\n",
    "        .master('local[*]')\n",
    "        # Just to show dataframes as tables\n",
    "        .config('spark.sql.repl.eagerEval.enabled', True)\n",
    "        # On DataProc we must use spark 2.x\n",
    "        .config('spark.jars.packages', \"org.neo4j:neo4j-connector-apache-spark_2.12:4.1.3_for_spark_2.4,com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.26.0\")\n",
    "        # These global credentials don't cascade on DataProc\n",
    "        .config('neo4j.url', neo4j_url)\n",
    "        .config('neo4j.authentication.type', \"basic\")\n",
    "        .config('neo4j.authentication.basic.username', neo4j_user)\n",
    "        .config('neo4j.authentication.basic.password', neo4j_password)\n",
    "        .getOrCreate())\n",
    "# output spark version\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's create a handle to the Graph Data Science library so we can make GDS calls and scripts more elegantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds = GraphDataScience(neo4j_url, auth=(neo4j_user, neo4j_password))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's now check that GDS is running on the server by executing this Cypher query.\n",
    "We only need to supply credentials once per notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"return gds.version() as gds_version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Query to see what data is there already using Spark Connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "    .option(\"url\", neo4j_url) \\\n",
    "    .option('authentication.type', \"basic\") \\\n",
    "    .option('authentication.basic.username', neo4j_user) \\\n",
    "    .option('authentication.basic.password', neo4j_password) \\\n",
    "    .option(\"labels\", \"MSA\").load().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If dataset is not empty, reset it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"CREATE OR REPLACE DATABASE `\"+neo4j_database+\"`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that it's empty now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"MATCH (n:MSA) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load MSA data from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First, let's try downloading a file from URL the Spark way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/smithna/datasets/main/CensusDemographicsByMetroArea.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "urlDf = spark.read.csv(\"file://\"+SparkFiles.get(\"CensusDemographicsByMetroArea.csv\"), header=True, inferSchema= True)\n",
    "urlDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check to see what kind of job Spark did inferring schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "urlDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We see that percentOver25WithBachelors is a double, which is not supported in Neo4j 4.x\n",
    "We can cast to float using selectExpr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "floatCastSelectExpr=[\"NAME as name\",\"population\",\\\n",
    "\"cast(PERCENTOVER25WITHBACHELORS as float) percentOver25WithBachelors\",\\\n",
    "\"cast(MEDIANHOMEPRICE as int) medianHomePrice\",\\\n",
    "\"cast(MEDIANHOUSEHOLDINCOME as int) medianHouseholdIncome\"]\n",
    "\n",
    "castUrlDf = urlDf.selectExpr(floatCastSelectExpr)\n",
    "castUrlDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now delete it, we are loading now from BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"CREATE OR REPLACE DATABASE `\"+neo4j_database+\"`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"MATCH (n:MSA) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load MSA data from BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This section leverages best practices described here: https://neo4j.com/docs/spark/current/dwh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create unique constraint on MSA.  The column \"name\" is the msa name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "CREATE CONSTRAINT msa_name IF NOT EXISTS ON (m:MSA) ASSERT m.name IS NODE KEY\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Create json template for reading and writing input from BigQuery. The data is here:\n",
    "\n",
    "    select name,\n",
    "       population,\n",
    "       medianHomePrice\n",
    "       percentOver25WithBachelors,\n",
    "       medianHouseholdIncome\n",
    "    from census.census_demos_by_msa"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BigQuery: Parallelized table scan\n",
    "The preferred approach for querying BigQuery is a simple table scan.\n",
    "This method supports maxParallelization if you want to throttle, otherwise it will maximize speed.\n",
    "\n",
    "https://github.com/GoogleCloudDataproc/spark-bigquery-connector/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# If we're doing a table scan, the options table syntax can be used and it provides multi-threading\n",
    "msa_table=\"\"\"\n",
    "    neo4jbusinessdev.census.msa_demos\n",
    "\"\"\"\n",
    "tableBqDf = spark.read.format(\"bigquery\").option(\"table\",msa_table).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's count the records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tableBqDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tableBqDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BigQuery: Post query filtering\n",
    "We loaded all MSAs in massive parallel, now we can filter down the recordset using Spark SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "metro_select=\"\"\"\n",
    "SELECT * FROM msa_df WHERE name LIKE \"%Metro%\"\n",
    "\"\"\"\n",
    "tableBqDf.createOrReplaceTempView(\"msa_df\")\n",
    "bigqueryPostQueryDf = spark.sql(metro_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bigqueryPostQueryDf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bigqueryPostQueryDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## BigQuery: SQL push-down, temporary materialization\n",
    "To push down a full SQL statement to BigQuery, we must materialize of results in a BigQuery dataset.\n",
    "The temporary materialization is used as a cache by BigQuery.  It will be deleted after 1 day by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tempLocation = \"dataproc_testing\"\n",
    "spark.conf.set(\"viewsEnabled\",\"true\")\n",
    "spark.conf.set(\"materializationDataset\",\"census\")\n",
    "# if this value is too low, big query will expect that it exists in cache.  Default is 1440.\n",
    "spark.conf.set(\"materializationExpirationTimeInMinutes\",5)\n",
    "\n",
    "msa_query_sql=\"\"\"\n",
    "    select * from `neo4jbusinessdev.census.msa_demos` where name LIKE \"%Metro%\"\n",
    "    \"\"\"\n",
    "bigqueryDf = spark.read.format(\"bigquery\").option(\"query\",msa_query_sql).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Count results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bigqueryDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bigqueryDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bigqueryDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's cast columns before committing to Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "floatCastSelectExpr=[\"NAME as name\",\"population\",\\\n",
    "\"cast(PERCENTOVER25WITHBACHELORS as float) percentOver25WithBachelors\",\\\n",
    "\"cast(MEDIANHOMEPRICE as int) medianHomePrice\",\\\n",
    "\"cast(MEDIANHOUSEHOLDINCOME as int) medianHouseholdIncome\"]\n",
    "\n",
    "castBigqueryDf = bigqueryDf.selectExpr(floatCastSelectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "castBigqueryDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The Neo4j Spark Connector makes it easy to load data from a DataFrame into Neo4j."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This write is Spark native\n",
    "(castBigqueryDf.write\n",
    "  .format(\"org.neo4j.spark.DataSource\")\n",
    "  .mode(\"Overwrite\")\n",
    "  .option(\"labels\", \":MSA\")\n",
    "  .option(\"node.keys\",\"name\")\n",
    "  .option(\"url\", neo4j_url) \n",
    "  .option(\"authentication.type\", \"basic\") \n",
    "  .option(\"authentication.basic.username\", neo4j_user) \n",
    "  .option(\"authentication.basic.password\", neo4j_password) \n",
    "  .option(\"partitions\", \"1\") \n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check for data now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"MATCH (n:MSA) RETURN count(n)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Read from Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4jDf=spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"labels\", \"MSA\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.type\", \"basic\")  \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user)  \\\n",
    "  .option(\"authentication.basic.password\", neo4j_password)  \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4jDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Cast double to long.  Renaming could be done in the same step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cast_sql=\"\"\"\n",
    "    select name, \n",
    "    cast(POPULATION as long) population,\n",
    "    cast(PERCENTOVER25WITHBACHELORS as float) percentOver25WithBachelors,\n",
    "    cast(MEDIANHOMEPRICE as int) medianHomePrice,\n",
    "    cast(MEDIANHOUSEHOLDINCOME as int) medianHouseholdIncome\n",
    "    from msa_df\n",
    "\"\"\"\n",
    "neo4jDf.createOrReplaceTempView(\"msa_df\")\n",
    "neo4jDfCasted = spark.sql(cast_sql)\n",
    "neo4jDfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert Spark dataframe to pandas to display histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pandas_msa_df=neo4jDfCasted.toPandas()\n",
    "print(pandas_msa_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now show histograms of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 2)\n",
    "fig.set_size_inches(15,30)\n",
    "for i in range(1,5):\n",
    "    sns.histplot(pandas_msa_df.iloc[:,i], ax=axes[i-1,0])\n",
    "    sns.histplot(pandas_msa_df.iloc[:,i], log_scale=True, ax=axes[i-1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "That log transformation looks like it will help. Run the Cypher to store the transformed values in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "log_new_view_sql = \"\"\"\n",
    "SELECT name, \n",
    "population, CAST(log(population)  as float) as logPopulation,\n",
    "medianHouseholdIncome,  CAST(log(medianHouseholdIncome) as float) as logMedianHouseholdIncome,\n",
    "medianHomePrice, CAST(log(medianHomePrice) as float) as logMedianHomePrice,\n",
    "CAST(percentOver25WithBachelors as float) as percentOver25WithBachelors, CAST(log(percentOver25WithBachelors) as float) as logPercentOver25WithBachelors\n",
    "FROM msa_df\n",
    "\"\"\"\n",
    "neo4jDfCasted.createOrReplaceTempView(\"msa_df\")\n",
    "neo4jDfLogUpdated = spark.sql(log_new_view_sql)\n",
    "neo4jDfLogUpdated.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Commit to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This write is Spark native\n",
    "(neo4jDfLogUpdated.write\n",
    "  .format(\"org.neo4j.spark.DataSource\")\n",
    "  .mode(\"Overwrite\")\n",
    "  .option(\"labels\", \":MSA\")\n",
    "  .option(\"node.keys\",\"name\")\n",
    "  .option(\"url\", neo4j_url) \n",
    "  .option(\"authentication.type\", \"basic\") \n",
    "  .option(\"authentication.basic.username\", neo4j_user) \n",
    "  .option(\"authentication.basic.password\", neo4j_password) \n",
    "  .option(\"partitions\", \"1\") \n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Look in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neo4jDf=spark.read.format(\"org.neo4j.spark.DataSource\") \\\n",
    "  .option(\"labels\", \"MSA\") \\\n",
    "  .option(\"url\", neo4j_url) \\\n",
    "  .option(\"authentication.type\", \"basic\")  \\\n",
    "  .option(\"authentication.basic.username\", neo4j_user)  \\\n",
    "  .option(\"authentication.basic.password\", neo4j_password)  \\\n",
    "  .load().printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Check that logs were committed to the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create in-memory graph projection\n",
    "Passing `\"*\"` as the third argument to `gds.graph.project` tells GDS to include any relationships that exist in the database in the in-memory graph. Because no relationships have been created in the graph yet, there will be no relationships in the in-memory graph projection when it is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "g_msa, result = gds.graph.project(\n",
    "    'msa-graph', \n",
    "    'MSA', \n",
    "    '*', \n",
    "    nodeProperties = [\n",
    "        \"logPopulation\", \n",
    "        \"logMedianHouseholdIncome\", \n",
    "        \"logMedianHomePrice\", \n",
    "        \"logPercentOver25WithBachelors\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notice that when we look at the results of gds.graph.project, we see that the relationshipCount is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Apply MinMax scalar to property values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.alpha.scaleProperties.mutate(g_msa, \n",
    "                                 nodeProperties = [\n",
    "                                     \"logPopulation\", \n",
    "                                     \"logMedianHouseholdIncome\", \n",
    "                                     \"logMedianHomePrice\", \n",
    "                                     \"logPercentOver25WithBachelors\"], \n",
    "                                 scaler = \"MinMax\",\n",
    "                                 mutateProperty = \"scaledProperties\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This next line streams node properties to the procedure caller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sp = gds.graph.streamNodeProperty(g_msa, \"scaledProperties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Cleanup resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,0].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,1].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,2].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(list(sp['propertyValue'])).iloc[:,3].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run KNN to create relationships to nearest neighbors\n",
    "First run in stats mode and look at the similarity distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_stats = gds.knn.stats(g_msa,\n",
    "                          nodeProperties={\"scaledProperties\":\"EUCLIDEAN\"},\n",
    "                          topK=15\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "knn_stats['similarityDistribution']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now run KNN in mutate mode to update the in-memory graph projection. We'll exclude the bottom 1% of similarity relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.knn.mutate(g_msa,\n",
    "               nodeProperties={\"scaledProperties\":\"EUCLIDEAN\"},\n",
    "               topK=15,\n",
    "               mutateRelationshipType=\"IS_SIMILAR\",\n",
    "               mutateProperty=\"similarity\",\n",
    "               similarityCutoff=knn_stats['similarityDistribution']['p1']\n",
    "              )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Also write the relationships from the in-memory graph projection back to the on-disk graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.graph.writeRelationship(\n",
    "    g_msa,\n",
    "    \"IS_SIMILAR\",\n",
    "    relationship_property=\"similarity\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Add a `rank` property to the `IS_SIMILAR` relationships for use with Bloom filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA)-[s:IS_SIMILAR]->()\n",
    "WITH m, s ORDER BY s.similarity DESC\n",
    "WITH m, collect(s) as similarities, range(0, 19) AS ranks\n",
    "UNWIND ranks AS rank\n",
    "WITH rank, similarities[rank] AS rel\n",
    "SET rel.rank = rank + 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Louvain Community Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "See how many communities Louvain is going to recommend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.louvain.stats(g_msa,\n",
    "                  relationshipTypes=[\"IS_SIMILAR\"],\n",
    "                 relationshipWeightProperty=\"similarity\"\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now commit louvain communities to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.louvain.write(g_msa,\n",
    "                  relationshipTypes=[\"IS_SIMILAR\"],\n",
    "                  relationshipWeightProperty =\"similarity\",\n",
    "                  writeProperty=\"communityId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Gather statistics about the communities that were discovered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Get average values for each community and 3 example MSAs for each community."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "communityDf = gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA)\n",
    "WITH m \n",
    "ORDER BY apoc.coll.sum([(m)-[s:IS_SIMILAR]->(m2) \n",
    "WHERE m.communityId = m2.communityId | s.similarity]) desc\n",
    "RETURN m.communityId as communityId,\n",
    "count(m) as msaCount, \n",
    "avg(m.population) as avgPopulation,\n",
    "avg(m.medianHomePrice) as avgHomePrice,\n",
    "avg(m.medianHouseholdIncome) as avgIncome,\n",
    "avg(m.percentOver25WithBachelors) as avgPctBachelors,\n",
    "collect(m.name)[..3] as exampleMSAs\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "communityDf.sort_values('communityId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,6):\n",
    "    sns.barplot(data=communityDf, x=\"communityId\", y=communityDf.columns[i], ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mean can give us a quick overview of properties, but can be skewed by outliers. Compare emperical cumulative distribution function (ECDF) at various proportions to get a more complete picture of distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "detailDf = gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA)\n",
    "RETURN \"community \" + m.communityId as communityId,\n",
    "m.population as population,\n",
    "m.medianHomePrice as medianHomePrice,\n",
    "m.medianHouseholdIncome as medianIncome,\n",
    "m.percentOver25WithBachelors as pctBachelors\n",
    "order by m.communityId\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(4, 1)\n",
    "fig.set_size_inches(6,20)\n",
    "for i in range(1,5):\n",
    "    sns.ecdfplot(data=detailDf, hue=\"communityId\", x=detailDf.columns[i], log_scale=True, ax=axes[i-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Compare two-dimensions on scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=detailDf, x=\"medianIncome\", y=\"population\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "splot = sns.scatterplot(data=detailDf, x=\"pctBachelors\", y=\"medianHomePrice\", hue=\"communityId\")\n",
    "splot.set(yscale=\"log\")\n",
    "splot.set(xscale=\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Write enriched features back to BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this section, we are going to write features back to BigQuery.  Let's begin by previewing a query that we will materialize to BigQuery.  Note that you can use SKIP and LIMIT in cypher when using the Spark connector since it is batching the query internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "relationship_cypher=\"\"\"\n",
    "MATCH (s)-[r:IS_SIMILAR]-(t) \n",
    "RETURN s.name AS msa_name\n",
    ",t.name AS related_name\n",
    ",r.similarity AS similarity\n",
    ",s.communityId AS louvain_community_id\n",
    "\"\"\"\n",
    "relsPandaDf=gds.run_cypher(relationship_cypher)\n",
    "relsPandaDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Convert pandas dataframe to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Create PySpark DataFrame from Pandas\n",
    "relsDf=spark.createDataFrame(relsPandaDf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now write to bigquery.  Note that writes require a temporary storage location for Avro files in process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark dataframe from \n",
    "\n",
    "# tmp_working_bucket\n",
    "spark.conf.set(\"temporaryGcsBucket\", \"neo4j_sandbox\")\n",
    "\n",
    "(relsDf.write.format(\"bigquery\")\n",
    "  .mode(\"overwrite\") \n",
    "  .option(\"table\",\"neo4jbusinessdev.census.msa_demo_similarity\")\n",
    "  .save())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now look for data in BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"bigquery\").option(\"table\",\"census.msa_demo_similarity\").load().show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Optional: assign human-friendly names to the clusters discovered.\n",
    "The Louvain community detection algorithm is not deterministic. You should have roughly the same clusters from previous runs, but some edge cases might be assigned to different communities. The community numbers might be shuffled between across different runs.  \n",
    "**This step requires adjustment by hand: choose from community IDs above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "MATCH (m:MSA) \n",
    "  SET m.communityName = CASE m.communityId \n",
    "  WHEN 54 THEN \"Large mid-cost metros\"\n",
    "  WHEN 75 THEN \"College towns\"\n",
    "  WHEN 81 THEN \"Large high-cost metros\"\n",
    "  WHEN 234 THEN \"Mid-size metros\"\n",
    "  WHEN 264 THEN \"Small metros\"\n",
    "  WHEN 330 THEN \"Mid-price metros\"\n",
    "  WHEN 385 THEN \"Low-income metros\"\n",
    "  END\n",
    "return m.communityName, m.communityId, count(*)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Create an index on the communityName property to make it searchable in Bloom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gds.run_cypher(\"\"\"\n",
    "CREATE INDEX msa_community_name IF NOT EXISTS\n",
    "FOR (m:MSA)\n",
    "ON (m.communityName)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now open Bloom and do some additional analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_project_drop = \"\"\"\n",
    "    CALL gds.graph.drop(\n",
    "    'msa-graph')\n",
    "\"\"\"\n",
    "gds.run_cypher(graph_project_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Drop BigQuery table.  There is no native commands to delete with spark so let's use the Python API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "# Construct a BigQuery client object.\n",
    "client = bigquery.Client()\n",
    "table_id = 'neo4jbusinessdev.census.msa_demo_similarity'\n",
    "# Will raisegoogle.api_core.exceptions.NotFound unless not_found_ok is True.\n",
    "client.delete_table(table_id, not_found_ok=True)  # Make an API request.\n",
    "print(\"Deleted table '{}'.\".format(table_id))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can verify deletion here...  Error is expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format(\"bigquery\").option(\"table\",\"census.msa_demo_similarity\").load().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}